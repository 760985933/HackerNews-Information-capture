#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ITèµ„è®¯æŠ“å–ç³»ç»Ÿä¸»ç¨‹åº
è´Ÿè´£åè°ƒå„ä¸ªæ¨¡å—ï¼Œæ‰§è¡ŒæŠ“å–ä»»åŠ¡
"""

import sys
import argparse
from typing import Optional, Dict, Any
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from backend.config.settings import settings
from backend.utils.logger import logger_manager
from backend.utils.file_manager import FileManager
from backend.crawlers.crawler_factory import CrawlerFactory
from backend.crawlers.hackernews_crawler import HackerNewsCrawler


class NewsAggregator:
    """
    æ–°é—»èšåˆå™¨ä¸»ç±»
    è´Ÿè´£åè°ƒæ•´ä¸ªæŠ“å–æµç¨‹
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–æ–°é—»èšåˆå™¨
        """
        # åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        self.logger = logger_manager.get_logger("main")
        
        # åˆå§‹åŒ–æ–‡ä»¶ç®¡ç†å™¨
        self.file_manager = FileManager(settings.OUTPUT_DIR)
        
        # æ³¨å†ŒæŠ“å–å™¨
        self._register_crawlers()
        
        self.logger.info("ITèµ„è®¯æŠ“å–ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _register_crawlers(self) -> None:
        """
        æ³¨å†Œæ‰€æœ‰å¯ç”¨çš„æŠ“å–å™¨
        """
        try:
            CrawlerFactory.register_crawler("hackernews", HackerNewsCrawler)
            self.logger.info("æŠ“å–å™¨æ³¨å†Œå®Œæˆ")
        except Exception as e:
            self.logger.error(f"æŠ“å–å™¨æ³¨å†Œå¤±è´¥: {str(e)}")
            raise
    
    def run_crawler(self, crawler_name: str, config: Optional[Dict[str, Any]] = None) -> bool:
        """
        è¿è¡ŒæŒ‡å®šçš„æŠ“å–å™¨
        
        :param crawler_name: æŠ“å–å™¨åç§°
        :param config: å¯é€‰çš„é…ç½®å‚æ•°
        :return: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"å¼€å§‹è¿è¡ŒæŠ“å–å™¨: {crawler_name}")
            
            # è·å–æŠ“å–å™¨é…ç½®
            crawler_config = settings.get_crawler_config(crawler_name)
            if config:
                crawler_config.update(config)
            
            # åˆ›å»ºæŠ“å–å™¨å®ä¾‹
            crawler = CrawlerFactory.create_crawler(crawler_name, crawler_config)
            if not crawler:
                self.logger.error(f"æ— æ³•åˆ›å»ºæŠ“å–å™¨: {crawler_name}")
                return False
            
            # éªŒè¯é…ç½®
            if not crawler.validate_config():
                self.logger.error(f"æŠ“å–å™¨é…ç½®æ— æ•ˆ: {crawler_name}")
                return False
            
            # æ‰§è¡ŒæŠ“å–
            with crawler:
                result = crawler.crawl()
                
                if not result.success:
                    self.logger.error(f"æŠ“å–å¤±è´¥: {result.error_message}")
                    return False
                
                # ä¿å­˜ç»“æœ
                output_file = self.file_manager.save_crawl_result(result)
                if output_file:
                    self.logger.info(f"æŠ“å–å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ°: {output_file}")
                    print(f"âœ… æŠ“å–å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
                    print(f"ğŸ“Š å…±æŠ“å– {result.total_count} ç¯‡æ–‡ç« ")
                    return True
                else:
                    self.logger.error("ä¿å­˜æŠ“å–ç»“æœå¤±è´¥")
                    return False
                    
        except Exception as e:
            self.logger.error(f"è¿è¡ŒæŠ“å–å™¨æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"âŒ æŠ“å–å¤±è´¥: {str(e)}")
            return False
    
    def list_crawlers(self) -> None:
        """
        åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æŠ“å–å™¨
        """
        crawlers = CrawlerFactory.get_available_crawlers()
        
        print("ğŸ“‹ å¯ç”¨çš„æŠ“å–å™¨:")
        print("-" * 50)
        
        for name, description in crawlers.items():
            print(f"  â€¢ {name}: {description}")
        
        print("-" * 50)
        print(f"æ€»è®¡: {len(crawlers)} ä¸ªæŠ“å–å™¨")
    
    def show_config(self) -> None:
        """
        æ˜¾ç¤ºå½“å‰é…ç½®
        """
        config = settings.to_dict()
        
        print("âš™ï¸  å½“å‰ç³»ç»Ÿé…ç½®:")
        print("-" * 50)
        
        for key, value in config.items():
            if key == 'CRAWLER_CONFIGS':
                print(f"  {key}:")
                for crawler_name, crawler_config in value.items():
                    print(f"    {crawler_name}:")
                    for config_key, config_value in crawler_config.items():
                        print(f"      {config_key}: {config_value}")
            else:
                print(f"  {key}: {value}")
        
        print("-" * 50)
    
    def cleanup_files(self, days: int = None) -> None:
        """
        æ¸…ç†æ—§æ–‡ä»¶
        
        :param days: ä¿ç•™å¤©æ•°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®å€¼
        """
        days = days or settings.CLEANUP_DAYS
        
        try:
            deleted_count = self.file_manager.cleanup_old_files(days)
            log_deleted_count = logger_manager.cleanup_old_logs(days)
            
            print(f"ğŸ§¹ æ¸…ç†å®Œæˆ:")
            print(f"  â€¢ åˆ é™¤äº† {deleted_count} ä¸ªæ—§çš„è¾“å‡ºæ–‡ä»¶")
            print(f"  â€¢ åˆ é™¤äº† {log_deleted_count} ä¸ªæ—§çš„æ—¥å¿—æ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            print(f"âŒ æ¸…ç†å¤±è´¥: {str(e)}")


def create_argument_parser() -> argparse.ArgumentParser:
    """
    åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨
    
    :return: å‚æ•°è§£æå™¨
    """
    parser = argparse.ArgumentParser(
        description="ITèµ„è®¯æŠ“å–ç³»ç»Ÿ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  python main.py run hackernews                    # è¿è¡ŒHacker NewsæŠ“å–å™¨
  python main.py run hackernews --delay 2          # è®¾ç½®è¯·æ±‚å»¶è¿Ÿä¸º2ç§’
  python main.py list                              # åˆ—å‡ºæ‰€æœ‰å¯ç”¨æŠ“å–å™¨
  python main.py config                            # æ˜¾ç¤ºå½“å‰é…ç½®
  python main.py cleanup                           # æ¸…ç†æ—§æ–‡ä»¶
  python main.py cleanup --days 7                  # æ¸…ç†7å¤©å‰çš„æ–‡ä»¶
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # run å‘½ä»¤
    run_parser = subparsers.add_parser('run', help='è¿è¡ŒæŠ“å–å™¨')
    run_parser.add_argument('crawler', help='æŠ“å–å™¨åç§°')
    run_parser.add_argument('--delay', type=int, help='è¯·æ±‚é—´å»¶è¿Ÿç§’æ•°')
    run_parser.add_argument('--max-pages', type=int, help='æœ€å¤§æŠ“å–é¡µæ•°')
    run_parser.add_argument('--fetch-content', action='store_true', help='æ˜¯å¦æŠ“å–æ–‡ç« å†…å®¹')
    run_parser.add_argument('--timeout', type=int, help='è¯·æ±‚è¶…æ—¶æ—¶é—´')
    
    # list å‘½ä»¤
    subparsers.add_parser('list', help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨æŠ“å–å™¨')
    
    # config å‘½ä»¤
    subparsers.add_parser('config', help='æ˜¾ç¤ºå½“å‰é…ç½®')
    
    # cleanup å‘½ä»¤
    cleanup_parser = subparsers.add_parser('cleanup', help='æ¸…ç†æ—§æ–‡ä»¶')
    cleanup_parser.add_argument('--days', type=int, help='ä¿ç•™å¤©æ•°')
    
    return parser


def main():
    """
    ä¸»å‡½æ•°
    """
    try:
        # è§£æå‘½ä»¤è¡Œå‚æ•°
        parser = create_argument_parser()
        args = parser.parse_args()
        
        if not args.command:
            parser.print_help()
            return
        
        # åˆ›å»ºæ–°é—»èšåˆå™¨å®ä¾‹
        aggregator = NewsAggregator()
        
        # æ‰§è¡Œç›¸åº”å‘½ä»¤
        if args.command == 'run':
            # æ„å»ºé…ç½®å‚æ•°
            config = {}
            if args.delay is not None:
                config['delay'] = args.delay
            if args.max_pages is not None:
                config['max_pages'] = args.max_pages
            if args.fetch_content:
                config['fetch_content'] = True
            if args.timeout is not None:
                config['timeout'] = args.timeout
            
            # è¿è¡ŒæŠ“å–å™¨
            success = aggregator.run_crawler(args.crawler, config if config else None)
            sys.exit(0 if success else 1)
            
        elif args.command == 'list':
            aggregator.list_crawlers()
            
        elif args.command == 'config':
            aggregator.show_config()
            
        elif args.command == 'cleanup':
            aggregator.cleanup_files(args.days)
            
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()